{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomNormal, Zeros, he_normal\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_network():\n",
    "    def __init__(self,sess,n_parts, input_dim, output_dim, \n",
    "                 activation = 'tanh', n_hidden =400,dropout = False,rate = 0.3, mu = 0, sigma = 1):\n",
    "        self.n_parts = n_parts\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.rate = rate\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.phi = tf.placeholder(tf.float32,shape=(n_parts,self.output_dim))\n",
    "        self.model = self.build_network()\n",
    "        \n",
    "        \n",
    "        self.eta_grad = tf.gradients(tf.stop_gradient(self.phi)*self.model.output,self.model.trainable_weights)\n",
    "    \n",
    "    \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        if self.dropout: \n",
    "            encoder_inputs = Input(batch_shape=(self.n_parts,self.input_dim))\n",
    "            encoder_indrop = Dropout(self.rate)(encoder_inputs)\n",
    "            encoder_hidden = Dense(self.n_hidden, activation=self.activation,\n",
    "                               kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                               bias_initializer = Zeros())(encoder_indrop)\n",
    "            encoder_hidden = Dropout(self.rate)(encoder_hidden)\n",
    "            encoder_output = Dense(self.output_dim,\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(encoder_hidden)\n",
    "            encoder = Model(inputs = encoder_inputs, outputs = encoder_output)\n",
    "        else:\n",
    "            encoder_inputs = Input(batch_shape=(self.n_parts,self.input_dim))\n",
    "            encoder_hidden = Dense(self.n_hidden, activation=self.activation,\n",
    "                               kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                               bias_initializer = Zeros())(encoder_inputs)\n",
    "            encoder_output = Dense(self.output_dim,\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(encoder_hidden)\n",
    "            encoder = Model(inputs = encoder_inputs, outputs = encoder_output)\n",
    "        \n",
    "        return encoder\n",
    "            \n",
    "    def eval_eta_grad(self,x_in,phi):\n",
    "        return self.sess.run(self.eta_grad, feed_dict={self.model.input : x_in,self.phi : phi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_network():\n",
    "    def __init__(self,sess,n_parts,input_dim,output_dim,\n",
    "                 activation = 'tanh', n_hidden = 400,dropout = False,rate = 0.3, mu = 0, sigma = 1):\n",
    "        self.n_parts = n_parts\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.rate = rate\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.model = self.build_network()\n",
    "        \n",
    "        self.x_real = tf.placeholder(tf.float32, shape=(None,self.output_dim))\n",
    "        self.logpxz = self.build_logpxz(self.x_real, self.model.output)\n",
    "        \n",
    "        self.theta_grad = tf.gradients(self.logpxz, self.model.trainable_weights)\n",
    "        self.z_grad = tf.gradients(self.logpxz, self.model.input)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        if self.dropout:\n",
    "            decoder_inputs = Input(batch_shape=(None, self.input_dim))\n",
    "            decoder_indrop = Dropout(self.rate)(decoder_inputs)\n",
    "            decoder_hidden = Dense(self.n_hidden,activation=self.activation,\n",
    "                               kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                               bias_initializer = Zeros())(decoder_indrop)\n",
    "            decoder_hidden = Dropout(self.rate)(decoder_hidden)\n",
    "            decoder_output = Dense(self.output_dim, activation='sigmoid',\n",
    "                       kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                       bias_initializer = Zeros())(decoder_hidden)\n",
    "            decoder = Model(inputs = decoder_inputs, outputs = decoder_output)\n",
    "        else:\n",
    "            decoder_inputs = Input(batch_shape=(None, self.input_dim))\n",
    "            decoder_hidden = Dense(self.n_hidden,activation=self.activation,\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(decoder_inputs)\n",
    "            decoder_output = Dense(self.output_dim, activation='sigmoid',\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(decoder_hidden)\n",
    "            decoder = Model(inputs = decoder_inputs, outputs = decoder_output)\n",
    "        return decoder        \n",
    "    \n",
    "    # grad with respect to theta of log p(x|z)\n",
    "    def eval_theta_grad(self,z,x_in):\n",
    "        return self.sess.run(self.theta_grad,feed_dict={self.model.input:z, self.x_real:x_in})\n",
    "\n",
    "    # grad with respect to z of log p(x|z)\n",
    "    def eval_z_grad(self,z,x_in):\n",
    "        return self.sess.run(self.z_grad,feed_dict={self.model.input:z, self.x_real:x_in})\n",
    "    \n",
    "    def eval_logpxz(self,z,x_in):\n",
    "        return self.sess.run(self.logpxz,feed_dict={self.model.input:z, self.x_real:x_in})\n",
    "    \n",
    "    def build_logpxz(self, X_real, x_rec):\n",
    "        #x_real = tf.reshape(x_real,(1,self.output_dim))\n",
    "        #X_real = tf.tile(x_real,(self.n_parts,1))\n",
    "        x_rec  = tf.reshape(x_rec,(self.n_parts,self.output_dim))\n",
    "        \n",
    "        fudge = 1e-15\n",
    "        \n",
    "        part1 = X_real * tf.log(tf.where(x_rec > fudge,x_rec,fudge * tf.ones_like(x_rec)))\n",
    "        part2 = (1-X_real) * tf.log(tf.where(1-x_rec > fudge,1-x_rec,fudge * tf.ones_like(x_rec)))\n",
    "        \n",
    "        logpxz = tf.reduce_sum(part1+part2, axis=0)\n",
    "        return logpxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x,h = -1):\n",
    "    dist_mat = distance.squareform(distance.pdist(x))**2 #use scipy package to calculate pairwise euclidean distance mat\n",
    "\n",
    "    if h < 0 : #as suggested in the paper, calculate bandwith like so\n",
    "        h = np.sqrt(0.5*np.median(dist_mat) / np.log(x.shape[0]+1))    \n",
    "    kxy = np.exp(-dist_mat / h**2 /2.) #rbf kernel formula\n",
    "    dkxy = -np.matmul(kxy , x) #first part of derivative of kxy\n",
    "    for i in range(x.shape[1]): #second part of derivative of kxy\n",
    "        dkxy[:,i] += x[:,i] * np.sum(kxy,axis=1)\n",
    "    dkxy /= h**2\n",
    "\n",
    "    return kxy, dkxy\n",
    "\n",
    "def svgd_kernel(x):\n",
    "    xx = tf.matmul(x,tf.transpose(x))\n",
    "    x2 = tf.reduce_sum(tf.square(x),axis=1)\n",
    "    x2e = tf.tile(tf.expand_dims(x2,1),(1,x.shape[0]))\n",
    "    dist_mat = x2e + tf.transpose(x2e) - 2 * xx #calculate pairwise squared distance \n",
    "    \n",
    "    #workaround for lack of tf.median\n",
    "    h = tf.contrib.distributions.percentile(dist_mat, 50.0, interpolation='lower')\n",
    "    h += tf.contrib.distributions.percentile(dist_mat, 50.0, interpolation='higher') #this is 2*median !\n",
    "    h = 0.5 *(h / 2.) / tf.log(x.shape[0].value +1.0) #formula for kernel width estimate\n",
    "    \n",
    "    kxy = tf.exp(-dist_mat / h / 2.) #kernel matrix\n",
    "    dxkxy = (-tf.matmul(kxy,x) + (tf.expand_dims(tf.reduce_sum(kxy,axis=1),1) * x)) / h#derivative of kxy\n",
    "    \n",
    "    return kxy, dxkxy\n",
    "\n",
    "\n",
    "#the distance matrixes differ by ~1e-13 on my test, the h's differ due to rounding after 1e-5, rest propagates from there\n",
    "#probably close enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_input(x_input, loc, scale, n_parts,noise_dim = 1,shift_zeros = False):\n",
    "    xsi = np.random.normal(loc,scale,size=(n_parts,noise_dim))\n",
    "    x_extended = np.tile(x_input,(n_parts,1))\n",
    "    if shift_zeros:\n",
    "        x_extended[x_extended == 0] = -1\n",
    "    assert np.shape(x_extended) == (n_parts, np.size(x_input))\n",
    "    return np.hstack((x_extended,xsi))\n",
    "\n",
    "\n",
    "def corrupt_input(x_input,n_parts,drop_rate, shift_zeros = True):\n",
    "    x_input = np.tile(x_input,(n_parts,1))\n",
    "    noise_zero = np.random.binomial(n = 1 , p = drop_rate,size = x_input.shape)\n",
    "    noise_one = np.random.binomial(n = 1, p = 1 - drop_rate,size = x_input.shape)\n",
    "    x_corrupt = x_input * noise_one + (1 - x_input) * noise_zero\n",
    "    if shift_zeros:\n",
    "        x_corrupt[x_corrupt == 0] = -1\n",
    "    return x_corrupt\n",
    "\n",
    "def dynamical_binarization(data):\n",
    "    return np.random.binomial(n = 1, p = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_f, y_train_f), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "num_samples = x_train_f.shape[0]\n",
    "num_val = 10000\n",
    "num_train = num_samples - num_val\n",
    "\n",
    "orig_dim = x_train_f.shape[1:] #useful for restoring original images\n",
    "flat_dim = orig_dim[0] * orig_dim[1] #output dimension\n",
    "\n",
    "\n",
    "shuffle_idx = np.random.permutation(num_samples)\n",
    "x_val = dynamical_binarization(x_train_f[shuffle_idx[:num_val]].copy().reshape(-1,flat_dim) / 255)\n",
    "y_val = y_train_f[shuffle_idx[:num_val]].copy()\n",
    "x_train = dynamical_binarization(x_train_f[shuffle_idx[num_val:]].copy().reshape(-1,flat_dim) / 255)\n",
    "y_train = y_train_f[shuffle_idx[num_val:]].copy()\n",
    "\n",
    "# x_train = dynamical_binarization(x_train_f[y_train_f == 0].reshape(-1,flat_dim) / 255)\n",
    "# x_val = dynamical_binarization(x_train_f[y_train_f == 1].reshape(-1,flat_dim) / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "iters = 50000\n",
    "n_parts = 5\n",
    "latent_dim = 2\n",
    "n_hidden = 400\n",
    "\n",
    "drop_rate = 0.3\n",
    "\n",
    "noise_dim = 0\n",
    "input_dim = flat_dim  + noise_dim #input dimension\n",
    "\n",
    "loc = 0\n",
    "scale = 1\n",
    "epsilon = 1e-4\n",
    "alpha = 0\n",
    "\n",
    "en_sig = 0.02\n",
    "de_sig = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#define the optimizer\n",
    "enc_opt = tf.train.AdamOptimizer(epsilon)\n",
    "dec_opt = tf.train.AdamOptimizer(epsilon)\n",
    "\n",
    "\n",
    "\n",
    "#define the encoder\n",
    "encoder_inputs = Input(batch_shape=(n_parts,input_dim))\n",
    "encoder_hidden = Dense(n_hidden, activation='elu',\n",
    "#                    kernel_initializer = RandomNormal(0, en_sig),\n",
    "                       kernel_initializer = he_normal(),\n",
    "                   bias_initializer = Zeros())(encoder_inputs)\n",
    "encoder_output = Dense(latent_dim,\n",
    "#                        kernel_initializer = RandomNormal(0,en_sig),\n",
    "                       kernel_initializer = he_normal(),\n",
    "                       bias_initializer = Zeros())(encoder_hidden)\n",
    "encoder = Model(inputs = encoder_inputs, outputs = encoder_output)\n",
    "    \n",
    "#define the decoder\n",
    "decoder_inputs = Input(batch_shape=(n_parts,latent_dim))\n",
    "decoder_hidden = Dense(n_hidden,activation='elu',\n",
    "                       kernel_initializer = he_normal(),\n",
    "#                        kernel_initializer = RandomNormal(0,de_sig),\n",
    "                       bias_initializer = Zeros())(decoder_inputs)\n",
    "decoder_output = Dense(flat_dim,activation= 'sigmoid',\n",
    "#                        kernel_initializer = RandomNormal(0,de_sig),\n",
    "                       kernel_initializer = he_normal(),\n",
    "                       bias_initializer = Zeros())(decoder_hidden)\n",
    "decoder = Model(inputs = decoder_inputs, outputs = decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "x_in = tf.placeholder(tf.float32,(n_parts, input_dim))\n",
    "x_real = tf.placeholder(tf.float32, (n_parts, flat_dim))\n",
    "\n",
    "z = encoder(x_in)\n",
    "x_rec = decoder(z)\n",
    "\n",
    "fudge = 1e-15\n",
    "part1 = x_real * tf.log(tf.where(x_rec > fudge,x_rec,fudge * tf.ones_like(x_rec)))\n",
    "part2 = (1-x_real) * tf.log(tf.where(1-x_rec > fudge,1-x_rec,fudge * tf.ones_like(x_rec)))\n",
    "logpxz = tf.reduce_sum(part1+part2, axis=0)\n",
    "\n",
    "z_grad = tf.gradients(logpxz, z)\n",
    "z_grad = tf.reshape(z_grad, (n_parts, latent_dim))\n",
    "kzy, dkzy = svgd_kernel(z)\n",
    "phi = (tf.matmul(kzy, z_grad - z) + (1. + alpha) * dkzy) / n_parts\n",
    "\n",
    "enc_update = enc_opt.minimize(-tf.stop_gradient(phi) * z,var_list = encoder.trainable_weights)\n",
    "dec_update = dec_opt.minimize(-logpxz/n_parts, var_list = decoder.trainable_weights)\n",
    "\n",
    "updates = [enc_update, dec_update]\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in range(iters):\n",
    "    t = t % x_train.shape[0]\n",
    "    x_input = corrupt_input(x_train[t],n_parts,drop_rate)\n",
    "    x_r = np.tile(x_train[t].reshape(1,flat_dim),(n_parts,1))\n",
    "    \n",
    "    sess.run(updates, feed_dict = {x_in : x_input, x_real: x_r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, zs = [],[]\n",
    "for i in range(500):\n",
    "    x_ev = corrupt_input(x_val[i],n_parts, drop_rate)\n",
    "    z_ev = sess.run(z,feed_dict = {x_in : x_ev})\n",
    "    bs.append(sess.run(x_rec, feed_dict={z : z_ev}))\n",
    "    zs.append(z_ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(3,4,figsize=(16,8))\n",
    "idx = 0\n",
    "for k in range(4):\n",
    "    ax[0,k].imshow(bs[idx][0].reshape(orig_dim))\n",
    "    ax[1,k].imshow(x_val[idx].reshape(orig_dim))\n",
    "    ax[2,k].imshow(zs[idx])\n",
    "    idx+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "encoder = encoder_network(sess, n_parts, input_dim, latent_dim,sigma = en_sig)\n",
    "decoder = decoder_network(sess, n_parts, latent_dim, flat_dim,sigma = de_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in range(iters):\n",
    "    t = t % x_train.shape[0]\n",
    "    if noise_dim > 0:\n",
    "        x_input = extend_input(x_train[t],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "    else:\n",
    "        x_input = corrupt_input(x_train[t],n_parts,drop_rate)\n",
    "    x_real = np.tile(x_train[t].reshape(1,flat_dim),(n_parts,1))\n",
    "#     x_real = x_input.copy()\n",
    "#     x_real[x_real == -1 ] = 0\n",
    "\n",
    "    z = encoder.model.predict(x_input,batch_size=n_parts)\n",
    "    \n",
    "    z_grad = decoder.eval_z_grad(z,x_real)\n",
    "    z_grad = np.array(z_grad).reshape(n_parts,latent_dim)\n",
    "\n",
    "    if n_parts != 1:\n",
    "        kzy, dkzy = rbf_kernel(z)\n",
    "        phi = (kzy @ (z_grad - z) + (1 + alpha) * dkzy) / n_parts\n",
    "    else:\n",
    "        phi = z_grad - z\n",
    "    \n",
    "    eta_grad = encoder.eval_eta_grad(x_input,phi)\n",
    "    \n",
    "    eta_1 = []\n",
    "    for i,eta in enumerate(encoder.model.get_weights()):\n",
    "        eta_1.append(eta + epsilon * eta_grad[i])\n",
    "    encoder.model.set_weights(eta_1)\n",
    "    \n",
    "    theta_grad = decoder.eval_theta_grad(z,x_real)\n",
    "\n",
    "    theta_1 = []\n",
    "    for i,theta in enumerate(decoder.model.get_weights()):\n",
    "        theta_1.append(theta + ((theta_grad[i]) / n_parts))\n",
    "\n",
    "    decoder.model.set_weights(theta_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if noise_dim > 0:\n",
    "    x_input = extend_input(x_train[q],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "else:\n",
    "    x_input = corrupt_input(x_train[q],n_parts,drop_rate)\n",
    "# x_real = x_input.copy()\n",
    "# x_real[x_real == -1 ] = 0\n",
    "x_real = np.tile(x_train[t].reshape(1,flat_dim),(n_parts,1))\n",
    "plt.imshow(x_input[0,:flat_dim].reshape(orig_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder.predict(x_input,batch_size=n_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(z)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_grad = decoder.eval_z_grad(z,x_real)\n",
    "z_grad = np.array(z_grad).reshape(n_parts,latent_dim)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(z_grad)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kzy, dkzy = rbf_kernel(z)\n",
    "# plt.scatter(dkzy[:,0],dkzy[:,1])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(dkzy)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(kzy)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blub = kzy @ (z_grad - z)\n",
    "#plt.scatter(blub[:,0],blub[:,1])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(blub)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = (kzy @ (z_grad - z) + (1 + 0) * dkzy) / n_parts\n",
    "#plt.scatter(phi[:,0],phi[:,1])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(phi)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_grad = encoder.eval_eta_grad(x_input,phi)\n",
    "fig,ax = plt.subplots(1,len(eta_grad),figsize=(16,4))\n",
    "fig.suptitle('eta_grad')\n",
    "for i,g in enumerate([eta.reshape(-1) for eta in eta_grad]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_1 = []\n",
    "fig,ax = plt.subplots(1,len(eta_grad),figsize=(16,4))\n",
    "fig.suptitle('eta new')\n",
    "for i,eta in enumerate(encoder.model.get_weights()):\n",
    "    step = eta + epsilon * eta_grad[i]\n",
    "    eta_1.append(step)\n",
    "    ax[i].hist(step.reshape(-1),bins=100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = encoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('eta old')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_grad = decoder.eval_theta_grad(z,x_real)\n",
    "fig,ax = plt.subplots(1,len(theta_grad),figsize=(16,4))\n",
    "fig.suptitle('theta_grad')\n",
    "for i,g in enumerate([theta.reshape(-1) for theta in theta_grad]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = decoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('theta old')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = []\n",
    "fig,ax = plt.subplots(1,len(theta_grad),figsize=(16,4))\n",
    "fig.suptitle('theta new')\n",
    "for i,theta in enumerate(decoder.model.get_weights()):\n",
    "    step = theta + ((epsilon*theta_grad[i]) / n_parts)\n",
    "    theta_1.append(step)\n",
    "    ax[i].hist(step.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.model.set_weights(theta_1)\n",
    "encoder.model.set_weights(eta_1)\n",
    "q+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rec = decoder.model.predict(z)[0]\n",
    "plt.imshow(x_rec.reshape(orig_dim))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =0\n",
    "plt.imshow(x_val[q].reshape(orig_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = extend_input(x_val[q],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "test = corrupt_input(x_val[q],n_parts, drop_rate = 0.01)\n",
    "test_shift = corrupt_input(x_val[q],n_parts, drop_rate = 0.01, shift_zeros = False)\n",
    "z = encoder.model.predict(test,batch_size=(n_parts))\n",
    "x_rec = decoder.model.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_rec[0].reshape(orig_dim))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = encoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('encoder weights')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)\n",
    "    \n",
    "fig,ax = plt.subplots(1,len(encoder.model.layers),figsize=(16,4))\n",
    "fig.suptitle('encoder activations')\n",
    "for i,layer in enumerate(encoder.model.layers):\n",
    "    activation = sess.run(layer.output,feed_dict={encoder.model.input:test})\n",
    "    if i < len(encoder.model.layers)-1:\n",
    "        ax[i].hist(activation.reshape(-1),bins=100)\n",
    "    else :\n",
    "        ax[i].hist(activation,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = decoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('decoder weights')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)\n",
    "    \n",
    "fig,ax = plt.subplots(1,len(decoder.model.layers),figsize=(16,4))\n",
    "fig.suptitle('decoder activations')\n",
    "for i,layer in enumerate(decoder.model.layers):\n",
    "    activation = sess.run(layer.output,feed_dict={decoder.model.input:z})\n",
    "    if i == 0:\n",
    "        ax[i].hist(activation)\n",
    "    else:\n",
    "        ax[i].hist(activation.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad_z = decoder.eval_z_grad(z,test_shift)\n",
    "test_grad_z = np.array(z_grad).reshape(n_parts,latent_dim)\n",
    "\n",
    "if n_parts != 1:\n",
    "    kzy, dkzy = rbf_kernel(z)\n",
    "    attraction = kzy @ (test_grad_z - z)\n",
    "    phi = (attraction + dkzy) / n_parts\n",
    "else:\n",
    "    phi = test_grad_z - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eta_grad =  encoder.eval_eta_grad(test,phi)\n",
    "fig,ax = plt.subplots(1,len(test_eta_grad),figsize=(16,4))\n",
    "fig.suptitle('eta_grad')\n",
    "for i,g in enumerate([eta.reshape(-1) for eta in test_eta_grad]):\n",
    "    ax[i].hist(g,bins=100)\n",
    "    \n",
    "test_theta_grad = decoder.eval_theta_grad(z,test_shift)\n",
    "fig,ax = plt.subplots(1,len(test_theta_grad),figsize=(16,4))\n",
    "fig.suptitle('theta_grad')\n",
    "for i,g in enumerate([theta.reshape(-1) for theta in test_theta_grad]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "colors = plt.cm.hsv(np.arange(10)/10)\n",
    "plt.figure(figsize=(12,8))\n",
    "for q in range(i):\n",
    "    if noise_dim > 0:\n",
    "        test_imgs = extend_input(x_val[q],loc,scale,n_parts = n_parts,noise_dim=noise_dim,shift_zeros=True)\n",
    "    else:\n",
    "        test_imgs = corrupt_input(x_val[q],n_parts, drop_rate)\n",
    "\n",
    "    test_z = encoder.model.predict(test_imgs,batch_size=(n_parts))\n",
    "    plt.scatter(test_z[:,0],test_z[:,1],c = colors[y_val[q]].reshape(1,4),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_eta = sess.run(tf.gradients(encoder.model.output,encoder.model.trainable_weights),\n",
    "                    feed_dict={encoder.model.input:test})\n",
    "plt.hist(pure_eta[0].reshape(-1),bins=100)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,len(pure_eta),figsize=(16,4))\n",
    "fig.suptitle('eta_grad')\n",
    "for i,g in enumerate([eta.reshape(-1) for eta in pure_eta]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_grad_z[:,0],test_grad_z[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,6))\n",
    "ax[0].scatter(phi[:,0],phi[:,1])\n",
    "ax[0].set_title('phi')\n",
    "ax[1].scatter(attraction[:,0],attraction[:,1])\n",
    "ax[1].set_title('attractive force')\n",
    "ax[2].scatter(dkzy[:,0],dkzy[:,1])\n",
    "ax[2].set_title('repulsive force')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def build_grad(self):\n",
    "#         grads = []\n",
    "#         for eta in self.model.trainable_weights:\n",
    "#             a = []\n",
    "#             for i in range(self.n_parts):\n",
    "#                 b = []\n",
    "#                 for j in range(self.output_dim):\n",
    "#                     b.append(tf.gradients(self.model.output[i,j],eta))\n",
    "#                 a.append(b)\n",
    "#             grads.append(a)\n",
    "#         return grads\n",
    "\n",
    "#     eta_1 = []\n",
    "#     for i,eta in enumerate(encoder.model.get_weights()):\n",
    "#         a = np.array(eta_grad[i]).reshape(n_parts,latent_dim,-1)\n",
    "#         b = np.array([np.matmul(phi[j,:].reshape(1,latent_dim),a[j,:,:]) for j in range(n_parts)])\n",
    "        \n",
    "#         eta_1.append(eta + epsilon * np.sum(b,axis=0).reshape(eta.shape))\n",
    "#     encoder.model.set_weights(eta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = x_val[q] * np.log(np.where(x_rec >= 1e-15,x_rec,np.ones(x_rec.shape)*1e-15)) \n",
    "+ (1-x_val[q]) * np.log(np.where(1 - x_rec >= 1e-15,1 - x_rec,1 - np.ones(x_rec.shape)*1e-15))\n",
    "np.mean(np.sum(np.where(x_val[q] == 1,x_val[q] * np.log(np.where(x_rec >= 1e-15,x_rec,np.ones(x_rec.shape)*1e-15)),\n",
    "       (1-x_val[q]) * np.log(np.where(1 - x_rec >= 1e-15,1 - x_rec,1 - np.ones(x_rec.shape)*1e-15))),axis=0))\n",
    "\n",
    "x_real = tf.placeholder(shape = (1,flat_dim,),dtype=tf.float32)\n",
    "X_real = tf.tile(x_real,(n_parts,1))\n",
    "x_rec  = tf.reshape(x_rec,(n_parts,flat_dim))\n",
    "\n",
    "fudge = 1e-15\n",
    "part1 = X_real * tf.log(tf.where(x_rec > fudge,x_rec,fudge * tf.ones_like(x_rec)))\n",
    "part2 = (1-X_real) * tf.log(tf.where(1-x_rec > fudge,1-x_rec,fudge * tf.ones_like(x_rec)))\n",
    "\n",
    "logpxz = tf.reduce_sum(part1+part2, axis=0)\n",
    "blubbers = sess.run(logpxz,feed_dict={x_real :x_val[q].reshape(1,flat_dim)})\n",
    "np.mean(blubbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat1 = x_val[q] * np.log(np.where(x_rec >= 1e-15,x_rec,np.ones(x_rec.shape)*1e-15)) \n",
    "wat2 = (1-x_val[q]) * np.log(np.where(1 - x_rec >= 1e-15,1 - x_rec,1 - np.ones(x_rec.shape)*1e-15))\n",
    "wat = wat1+wat2\n",
    "wat = np.sum(wat,axis=0)\n",
    "loss = 0\n",
    "for q in range(num_val):    \n",
    "    test = extend_input(x_val[q],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "    z = encoder.model.predict(test,batch_size=(n_parts))\n",
    "    test_logp=decoder.eval_logpxz(z,x_val[q])\n",
    "    loss += np.mean(test_logp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
