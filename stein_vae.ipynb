{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomNormal, Zeros\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_network():\n",
    "    def __init__(self,sess,n_parts, input_dim, output_dim, \n",
    "                 activation = 'tanh', n_hidden =400,dropout = False,rate = 0.3, mu = 0, sigma = 1):\n",
    "        self.n_parts = n_parts\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.rate = rate\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.phi = tf.placeholder(tf.float32,shape=(n_parts,self.output_dim))\n",
    "        self.model = self.build_network()\n",
    "        \n",
    "        \n",
    "        self.eta_grad = tf.gradients(tf.stop_gradient(self.phi)*self.model.output,self.model.trainable_weights)\n",
    "    \n",
    "    \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        if self.dropout: \n",
    "            encoder_inputs = Input(batch_shape=(self.n_parts,self.input_dim))\n",
    "            encoder_indrop = Dropout(self.rate)(encoder_inputs)\n",
    "            encoder_hidden = Dense(self.n_hidden, activation=self.activation,\n",
    "                               kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                               bias_initializer = Zeros())(encoder_indrop)\n",
    "            encoder_hidden = Dropout(self.rate)(encoder_hidden)\n",
    "            encoder_output = Dense(self.output_dim,\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(encoder_hidden)\n",
    "            encoder = Model(inputs = encoder_inputs, outputs = encoder_output)\n",
    "        else:\n",
    "            encoder_inputs = Input(batch_shape=(self.n_parts,self.input_dim))\n",
    "            encoder_hidden = Dense(self.n_hidden, activation=self.activation,\n",
    "                               kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                               bias_initializer = Zeros())(encoder_inputs)\n",
    "            encoder_output = Dense(self.output_dim,\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(encoder_hidden)\n",
    "            encoder = Model(inputs = encoder_inputs, outputs = encoder_output)\n",
    "        \n",
    "        return encoder\n",
    "            \n",
    "    def eval_eta_grad(self,x_in,phi):\n",
    "        return self.sess.run(self.eta_grad, feed_dict={self.model.input : x_in,self.phi : phi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_network():\n",
    "    def __init__(self,sess,n_parts,input_dim,output_dim,\n",
    "                 activation = 'tanh', n_hidden = 400,dropout = False,rate = 0.3, mu = 0, sigma = 1):\n",
    "        self.n_parts = n_parts\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.rate = rate\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.model = self.build_network()\n",
    "        \n",
    "        self.x_real = tf.placeholder(tf.float32, shape=(None,self.output_dim))\n",
    "        self.logpxz = self.build_logpxz(self.x_real, self.model.output)\n",
    "        \n",
    "        self.theta_grad = tf.gradients(self.logpxz, self.model.trainable_weights)\n",
    "        self.z_grad = tf.gradients(self.logpxz, self.model.input)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        if self.dropout:\n",
    "            decoder_inputs = Input(batch_shape=(None, self.input_dim))\n",
    "            decoder_indrop = Dropout(self.rate)(decoder_inputs)\n",
    "            decoder_hidden = Dense(self.n_hidden,activation=self.activation,\n",
    "                               kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                               bias_initializer = Zeros())(decoder_indrop)\n",
    "            decoder_hidden = Dropout(self.rate)(decoder_hidden)\n",
    "            decoder_output = Dense(self.output_dim, activation='sigmoid',\n",
    "                       kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                       bias_initializer = Zeros())(decoder_hidden)\n",
    "            decoder = Model(inputs = decoder_inputs, outputs = decoder_output)\n",
    "        else:\n",
    "            decoder_inputs = Input(batch_shape=(None, self.input_dim))\n",
    "            decoder_hidden = Dense(self.n_hidden,activation=self.activation,\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(decoder_inputs)\n",
    "            decoder_output = Dense(self.output_dim, activation='sigmoid',\n",
    "                                   kernel_initializer = RandomNormal(self.mu,self.sigma),\n",
    "                                   bias_initializer = Zeros())(decoder_hidden)\n",
    "            decoder = Model(inputs = decoder_inputs, outputs = decoder_output)\n",
    "        return decoder        \n",
    "    \n",
    "    # grad with respect to theta of log p(x|z)\n",
    "    def eval_theta_grad(self,z,x_in):\n",
    "        return self.sess.run(self.theta_grad,feed_dict={self.model.input:z, self.x_real:x_in})\n",
    "\n",
    "    # grad with respect to z of log p(x|z)\n",
    "    def eval_z_grad(self,z,x_in):\n",
    "        return self.sess.run(self.z_grad,feed_dict={self.model.input:z, self.x_real:x_in})\n",
    "    \n",
    "    def eval_logpxz(self,z,x_in):\n",
    "        return self.sess.run(self.logpxz,feed_dict={self.model.input:z, self.x_real:x_in})\n",
    "    \n",
    "    def build_logpxz(self, X_real, x_rec):\n",
    "        #x_real = tf.reshape(x_real,(1,self.output_dim))\n",
    "        #X_real = tf.tile(x_real,(self.n_parts,1))\n",
    "        x_rec  = tf.reshape(x_rec,(self.n_parts,self.output_dim))\n",
    "        \n",
    "        fudge = 1e-15\n",
    "        \n",
    "        part1 = X_real * tf.log(tf.where(x_rec > fudge,x_rec,fudge * tf.ones_like(x_rec)))\n",
    "        part2 = (1-X_real) * tf.log(tf.where(1-x_rec > fudge,1-x_rec,fudge * tf.ones_like(x_rec)))\n",
    "        \n",
    "        logpxz = tf.reduce_sum(part1+part2, axis=0)\n",
    "        return logpxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(z.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x,h = -1):\n",
    "    dist_mat = distance.squareform(distance.pdist(x))**2 #use scipy package to calculate pairwise euclidean distance mat\n",
    "\n",
    "    if h < 0 : #as suggested in the paper, calculate bandwith like so\n",
    "        h = np.sqrt(0.5*np.median(dist_mat) / np.log(x.shape[0]+1))    \n",
    "    kxy = np.exp(-dist_mat / h**2 /2.) #rbf kernel formula\n",
    "    dkxy = -np.matmul(kxy , x) #first part of derivative of kxy\n",
    "    for i in range(x.shape[1]): #second part of derivative of kxy\n",
    "        dkxy[:,i] += x[:,i] * np.sum(kxy,axis=1)\n",
    "    dkxy /= h**2\n",
    "\n",
    "    return kxy, dkxy\n",
    "\n",
    "def svgd_kernel(x):\n",
    "    xx = tf.matmul(x,tf.transpose(x))\n",
    "    x2 = tf.reduce_sum(tf.square(x),axis=1)\n",
    "    x2e = tf.tile(tf.expand_dims(x2,1),(1,x.shape[0]))\n",
    "    dist_mat = x2e + tf.transpose(x2e) - 2 * xx #calculate pairwise squared distance \n",
    "    \n",
    "    #workaround for lack of tf.median\n",
    "    h = tf.contrib.distributions.percentile(dist_mat, 50.0, interpolation='lower')\n",
    "    h += tf.contrib.distributions.percentile(dist_mat, 50.0, interpolation='higher') #this is 2*median !\n",
    "    h = tf.sqrt(0.5 *(h / 2.) / tf.log(x.shape[0].value +1.0)) #formula for kernel width estimate\n",
    "    \n",
    "    kxy = tf.exp(-dist_mat / h**2 / 2.) #kernel matrix\n",
    "    dxkxy = (-tf.matmul(kxy,x) + (tf.expand_dims(tf.reduce_sum(kxy,axis=1),1) * x)) / h**2 #derivative of kxy\n",
    "    \n",
    "    return kxy, dxkxy\n",
    "\n",
    "\n",
    "#the distance matrixes differ by ~1e-13 on my test, the h's differ due to rounding after 1e-5, rest propagates from there\n",
    "#probably close enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_input(x_input, loc, scale, n_parts,noise_dim = 1,shift_zeros = False):\n",
    "    xsi = np.random.normal(loc,scale,size=(n_parts,noise_dim))\n",
    "    x_extended = np.tile(x_input,(n_parts,1))\n",
    "    if shift_zeros:\n",
    "        x_extended[x_extended == 0] = -1\n",
    "    assert np.shape(x_extended) == (n_parts, np.size(x_input))\n",
    "    return np.hstack((x_extended,xsi))\n",
    "\n",
    "\n",
    "def corrupt_input(x_input,n_parts,drop_rate, shift_zeros = True):\n",
    "    x_input = np.tile(x_input,(n_parts,1))\n",
    "    noise_zero = np.random.binomial(n = 1 , p = drop_rate,size = x_input.shape)\n",
    "    noise_one = np.random.binomial(n = 1, p = 1 - drop_rate,size = x_input.shape)\n",
    "    x_corrupt = x_input * noise_one + (1 - x_input) * noise_zero\n",
    "    if shift_zeros:\n",
    "        x_corrupt[x_corrupt == 0] = -1\n",
    "    return x_corrupt\n",
    "\n",
    "def dynamical_binarization(data):\n",
    "    return np.random.binomial(n = 1, p = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_f, y_train_f), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "num_samples = x_train_f.shape[0]\n",
    "num_val = 10000\n",
    "num_train = num_samples - num_val\n",
    "\n",
    "orig_dim = x_train_f.shape[1:] #useful for restoring original images\n",
    "flat_dim = orig_dim[0] * orig_dim[1] #output dimension\n",
    "\n",
    "\n",
    "shuffle_idx = np.random.permutation(num_samples)\n",
    "x_val = dynamical_binarization(x_train_f[shuffle_idx[:num_val]].copy().reshape(-1,flat_dim) / 255)\n",
    "y_val = y_train_f[shuffle_idx[:num_val]].copy()\n",
    "x_train = dynamical_binarization(x_train_f[shuffle_idx[num_val:]].copy().reshape(-1,flat_dim) / 255)\n",
    "y_train = y_train_f[shuffle_idx[num_val:]].copy()\n",
    "\n",
    "# x_train = dynamical_binarization(x_train_f[y_train_f == 0].reshape(-1,flat_dim) / 255)\n",
    "# x_val = dynamical_binarization(x_train_f[y_train_f == 1].reshape(-1,flat_dim) / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "iters = 10000\n",
    "n_parts = 20\n",
    "latent_dim = 12\n",
    "n_hidden = 400\n",
    "\n",
    "drop_rate = 0.01\n",
    "\n",
    "noise_dim = 0\n",
    "input_dim = flat_dim  + noise_dim #input dimension\n",
    "\n",
    "loc = 0\n",
    "scale = 1\n",
    "epsilon = 1e-4\n",
    "alpha = 0\n",
    "\n",
    "en_sig = 0.02\n",
    "de_sig = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#define the optimizer\n",
    "enc_opt = tf.train.AdamOptimizer(epsilon)\n",
    "dec_opt = tf.train.AdamOptimizer(epsilon)\n",
    "\n",
    "\n",
    "\n",
    "#define the encoder\n",
    "encoder_inputs = Input(batch_shape=(n_parts,input_dim))\n",
    "encoder_hidden = Dense(n_hidden, activation='elu',\n",
    "                   kernel_initializer = RandomNormal(0, en_sig),\n",
    "                   bias_initializer = Zeros())(encoder_inputs)\n",
    "encoder_output = Dense(latent_dim,\n",
    "                       kernel_initializer = RandomNormal(0,en_sig),\n",
    "                       bias_initializer = Zeros())(encoder_hidden)\n",
    "encoder = Model(inputs = encoder_inputs, outputs = encoder_output)\n",
    "    \n",
    "#define the decoder\n",
    "decoder_inputs = Input(batch_shape=(n_parts,latent_dim))\n",
    "decoder_hidden = Dense(n_hidden,activation='elu',\n",
    "                       kernel_initializer = RandomNormal(0,de_sig),\n",
    "                       bias_initializer = Zeros())(decoder_inputs)\n",
    "decoder_output = Dense(flat_dim,activation= 'sigmoid',\n",
    "                       kernel_initializer = RandomNormal(0,de_sig),\n",
    "                       bias_initializer = Zeros())(decoder_hidden)\n",
    "decoder = Model(inputs = decoder_inputs, outputs = decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "x_in = tf.placeholder(tf.float32,(n_parts, input_dim))\n",
    "x_real = tf.placeholder(tf.float32, (x_rec.shape))\n",
    "\n",
    "z = encoder(x_in)\n",
    "x_rec = decoder(z)\n",
    "\n",
    "fudge = 1e-15\n",
    "part1 = x_real * tf.log(tf.where(x_rec > fudge,x_rec,fudge * tf.ones_like(x_rec)))\n",
    "part2 = (1-x_real) * tf.log(tf.where(1-x_rec > fudge,1-x_rec,fudge * tf.ones_like(x_rec)))\n",
    "logpxz = tf.reduce_sum(part1+part2, axis=0)\n",
    "\n",
    "z_grad = tf.gradients(logpxz, z)\n",
    "z_grad = tf.reshape(z_grad, (n_parts, latent_dim))\n",
    "kzy, dkzy = svgd_kernel(z)\n",
    "phi = (tf.matmul(kzy, z_grad - z) + (1. + alpha) * dkzy) / n_parts\n",
    "\n",
    "enc_update = enc_opt.minimize(tf.stop_gradient(phi) * z,var_list = encoder.trainable_weights)\n",
    "dec_update = dec_opt.minimize(-logpxz/n_parts, var_list = decoder.trainable_weights)\n",
    "\n",
    "updates = [enc_update, dec_update]\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iters):\n",
    "    t = t % x_train.shape[0]\n",
    "    x_input = corrupt_input(x_train[t],n_parts,drop_rate)\n",
    "    x_r = np.tile(x_train[t].reshape(1,flat_dim),(n_parts,1))\n",
    "    \n",
    "    sess.run(updates, feed_dict = {x_in : x_input, x_real: x_r})\n",
    "\n",
    "    if t % 200 == 0:\n",
    "        print(sess.run(logpxz, feed_dict = {x_in : x_input, x_real: x_r}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ev = sess.run(z,feed_dict = {x_in : x_input, x_real: x_r})\n",
    "plt.imshow(z_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "encoder = encoder_network(sess, n_parts, input_dim, latent_dim,sigma = en_sig)\n",
    "decoder = decoder_network(sess, n_parts, latent_dim, flat_dim,sigma = de_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in range(iters):\n",
    "    t = t % x_train.shape[0]\n",
    "    if noise_dim > 0:\n",
    "        x_input = extend_input(x_train[t],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "    else:\n",
    "        x_input = corrupt_input(x_train[t],n_parts,drop_rate)\n",
    "    x_real = np.tile(x_train[t].reshape(1,flat_dim),(n_parts,1))\n",
    "#     x_real = x_input.copy()\n",
    "#     x_real[x_real == -1 ] = 0\n",
    "\n",
    "    z = encoder.model.predict(x_input,batch_size=n_parts)\n",
    "    \n",
    "    z_grad = decoder.eval_z_grad(z,x_real)\n",
    "    z_grad = np.array(z_grad).reshape(n_parts,latent_dim)\n",
    "\n",
    "    if n_parts != 1:\n",
    "        kzy, dkzy = rbf_kernel(z)\n",
    "        phi = (kzy @ (z_grad - z) + (1 + alpha) * dkzy) / n_parts\n",
    "    else:\n",
    "        phi = z_grad - z\n",
    "    \n",
    "    eta_grad = encoder.eval_eta_grad(x_input,phi)\n",
    "    \n",
    "    eta_1 = []\n",
    "    for i,eta in enumerate(encoder.model.get_weights()):\n",
    "        eta_1.append(eta + epsilon * eta_grad[i])\n",
    "    encoder.model.set_weights(eta_1)\n",
    "    \n",
    "    theta_grad = decoder.eval_theta_grad(z,x_real)\n",
    "\n",
    "    theta_1 = []\n",
    "    for i,theta in enumerate(decoder.model.get_weights()):\n",
    "        theta_1.append(theta + ((theta_grad[i]) / n_parts))\n",
    "\n",
    "    decoder.model.set_weights(theta_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if noise_dim > 0:\n",
    "    x_input = extend_input(x_train[q],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "else:\n",
    "    x_input = corrupt_input(x_train[q],n_parts,drop_rate)\n",
    "# x_real = x_input.copy()\n",
    "# x_real[x_real == -1 ] = 0\n",
    "x_real = np.tile(x_train[t].reshape(1,flat_dim),(n_parts,1))\n",
    "plt.imshow(x_input[0,:flat_dim].reshape(orig_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder.predict(x_input,batch_size=n_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(z)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_grad = decoder.eval_z_grad(z,x_real)\n",
    "z_grad = np.array(z_grad).reshape(n_parts,latent_dim)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(z_grad)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kzy, dkzy = rbf_kernel(z)\n",
    "# plt.scatter(dkzy[:,0],dkzy[:,1])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(dkzy)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(kzy)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blub = kzy @ (z_grad - z)\n",
    "#plt.scatter(blub[:,0],blub[:,1])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(blub)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = (kzy @ (z_grad - z) + (1 + 0) * dkzy) / n_parts\n",
    "#plt.scatter(phi[:,0],phi[:,1])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(phi)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_grad = encoder.eval_eta_grad(x_input,phi)\n",
    "fig,ax = plt.subplots(1,len(eta_grad),figsize=(16,4))\n",
    "fig.suptitle('eta_grad')\n",
    "for i,g in enumerate([eta.reshape(-1) for eta in eta_grad]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_1 = []\n",
    "fig,ax = plt.subplots(1,len(eta_grad),figsize=(16,4))\n",
    "fig.suptitle('eta new')\n",
    "for i,eta in enumerate(encoder.model.get_weights()):\n",
    "    step = eta + epsilon * eta_grad[i]\n",
    "    eta_1.append(step)\n",
    "    ax[i].hist(step.reshape(-1),bins=100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = encoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('eta old')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_grad = decoder.eval_theta_grad(z,x_real)\n",
    "fig,ax = plt.subplots(1,len(theta_grad),figsize=(16,4))\n",
    "fig.suptitle('theta_grad')\n",
    "for i,g in enumerate([theta.reshape(-1) for theta in theta_grad]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = decoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('theta old')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = []\n",
    "fig,ax = plt.subplots(1,len(theta_grad),figsize=(16,4))\n",
    "fig.suptitle('theta new')\n",
    "for i,theta in enumerate(decoder.model.get_weights()):\n",
    "    step = theta + ((epsilon*theta_grad[i]) / n_parts)\n",
    "    theta_1.append(step)\n",
    "    ax[i].hist(step.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.model.set_weights(theta_1)\n",
    "encoder.model.set_weights(eta_1)\n",
    "q+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rec = decoder.model.predict(z)[0]\n",
    "plt.imshow(x_rec.reshape(orig_dim))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =0\n",
    "plt.imshow(x_val[q].reshape(orig_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = extend_input(x_val[q],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "test = corrupt_input(x_val[q],n_parts, drop_rate = 0.01)\n",
    "test_shift = corrupt_input(x_val[q],n_parts, drop_rate = 0.01, shift_zeros = False)\n",
    "z = encoder.model.predict(test,batch_size=(n_parts))\n",
    "x_rec = decoder.model.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_rec[0].reshape(orig_dim))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = encoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('encoder weights')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)\n",
    "    \n",
    "fig,ax = plt.subplots(1,len(encoder.model.layers),figsize=(16,4))\n",
    "fig.suptitle('encoder activations')\n",
    "for i,layer in enumerate(encoder.model.layers):\n",
    "    activation = sess.run(layer.output,feed_dict={encoder.model.input:test})\n",
    "    if i < len(encoder.model.layers)-1:\n",
    "        ax[i].hist(activation.reshape(-1),bins=100)\n",
    "    else :\n",
    "        ax[i].hist(activation,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = decoder.model.get_weights()\n",
    "fig,ax = plt.subplots(1,len(weights),figsize=(16,4))\n",
    "fig.suptitle('decoder weights')\n",
    "for i,weight in enumerate(weights):\n",
    "    ax[i].hist(weight.reshape(-1),bins=100)\n",
    "    \n",
    "fig,ax = plt.subplots(1,len(decoder.model.layers),figsize=(16,4))\n",
    "fig.suptitle('decoder activations')\n",
    "for i,layer in enumerate(decoder.model.layers):\n",
    "    activation = sess.run(layer.output,feed_dict={decoder.model.input:z})\n",
    "    if i == 0:\n",
    "        ax[i].hist(activation)\n",
    "    else:\n",
    "        ax[i].hist(activation.reshape(-1),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad_z = decoder.eval_z_grad(z,test_shift)\n",
    "test_grad_z = np.array(z_grad).reshape(n_parts,latent_dim)\n",
    "\n",
    "if n_parts != 1:\n",
    "    kzy, dkzy = rbf_kernel(z)\n",
    "    attraction = kzy @ (test_grad_z - z)\n",
    "    phi = (attraction + dkzy) / n_parts\n",
    "else:\n",
    "    phi = test_grad_z - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eta_grad =  encoder.eval_eta_grad(test,phi)\n",
    "fig,ax = plt.subplots(1,len(test_eta_grad),figsize=(16,4))\n",
    "fig.suptitle('eta_grad')\n",
    "for i,g in enumerate([eta.reshape(-1) for eta in test_eta_grad]):\n",
    "    ax[i].hist(g,bins=100)\n",
    "    \n",
    "test_theta_grad = decoder.eval_theta_grad(z,test_shift)\n",
    "fig,ax = plt.subplots(1,len(test_theta_grad),figsize=(16,4))\n",
    "fig.suptitle('theta_grad')\n",
    "for i,g in enumerate([theta.reshape(-1) for theta in test_theta_grad]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "colors = plt.cm.hsv(np.arange(10)/10)\n",
    "plt.figure(figsize=(12,8))\n",
    "for q in range(i):\n",
    "    if noise_dim > 0:\n",
    "        test_imgs = extend_input(x_val[q],loc,scale,n_parts = n_parts,noise_dim=noise_dim,shift_zeros=True)\n",
    "    else:\n",
    "        test_imgs = corrupt_input(x_val[q],n_parts, drop_rate)\n",
    "\n",
    "    test_z = encoder.model.predict(test_imgs,batch_size=(n_parts))\n",
    "    plt.scatter(test_z[:,0],test_z[:,1],c = colors[y_val[q]].reshape(1,4),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_eta = sess.run(tf.gradients(encoder.model.output,encoder.model.trainable_weights),\n",
    "                    feed_dict={encoder.model.input:test})\n",
    "plt.hist(pure_eta[0].reshape(-1),bins=100)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,len(pure_eta),figsize=(16,4))\n",
    "fig.suptitle('eta_grad')\n",
    "for i,g in enumerate([eta.reshape(-1) for eta in pure_eta]):\n",
    "    ax[i].hist(g,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_grad_z[:,0],test_grad_z[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,6))\n",
    "ax[0].scatter(phi[:,0],phi[:,1])\n",
    "ax[0].set_title('phi')\n",
    "ax[1].scatter(attraction[:,0],attraction[:,1])\n",
    "ax[1].set_title('attractive force')\n",
    "ax[2].scatter(dkzy[:,0],dkzy[:,1])\n",
    "ax[2].set_title('repulsive force')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def build_grad(self):\n",
    "#         grads = []\n",
    "#         for eta in self.model.trainable_weights:\n",
    "#             a = []\n",
    "#             for i in range(self.n_parts):\n",
    "#                 b = []\n",
    "#                 for j in range(self.output_dim):\n",
    "#                     b.append(tf.gradients(self.model.output[i,j],eta))\n",
    "#                 a.append(b)\n",
    "#             grads.append(a)\n",
    "#         return grads\n",
    "\n",
    "#     eta_1 = []\n",
    "#     for i,eta in enumerate(encoder.model.get_weights()):\n",
    "#         a = np.array(eta_grad[i]).reshape(n_parts,latent_dim,-1)\n",
    "#         b = np.array([np.matmul(phi[j,:].reshape(1,latent_dim),a[j,:,:]) for j in range(n_parts)])\n",
    "        \n",
    "#         eta_1.append(eta + epsilon * np.sum(b,axis=0).reshape(eta.shape))\n",
    "#     encoder.model.set_weights(eta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = x_val[q] * np.log(np.where(x_rec >= 1e-15,x_rec,np.ones(x_rec.shape)*1e-15)) \n",
    "+ (1-x_val[q]) * np.log(np.where(1 - x_rec >= 1e-15,1 - x_rec,1 - np.ones(x_rec.shape)*1e-15))\n",
    "np.mean(np.sum(np.where(x_val[q] == 1,x_val[q] * np.log(np.where(x_rec >= 1e-15,x_rec,np.ones(x_rec.shape)*1e-15)),\n",
    "       (1-x_val[q]) * np.log(np.where(1 - x_rec >= 1e-15,1 - x_rec,1 - np.ones(x_rec.shape)*1e-15))),axis=0))\n",
    "\n",
    "x_real = tf.placeholder(shape = (1,flat_dim,),dtype=tf.float32)\n",
    "X_real = tf.tile(x_real,(n_parts,1))\n",
    "x_rec  = tf.reshape(x_rec,(n_parts,flat_dim))\n",
    "\n",
    "fudge = 1e-15\n",
    "part1 = X_real * tf.log(tf.where(x_rec > fudge,x_rec,fudge * tf.ones_like(x_rec)))\n",
    "part2 = (1-X_real) * tf.log(tf.where(1-x_rec > fudge,1-x_rec,fudge * tf.ones_like(x_rec)))\n",
    "\n",
    "logpxz = tf.reduce_sum(part1+part2, axis=0)\n",
    "blubbers = sess.run(logpxz,feed_dict={x_real :x_val[q].reshape(1,flat_dim)})\n",
    "np.mean(blubbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat1 = x_val[q] * np.log(np.where(x_rec >= 1e-15,x_rec,np.ones(x_rec.shape)*1e-15)) \n",
    "wat2 = (1-x_val[q]) * np.log(np.where(1 - x_rec >= 1e-15,1 - x_rec,1 - np.ones(x_rec.shape)*1e-15))\n",
    "wat = wat1+wat2\n",
    "wat = np.sum(wat,axis=0)\n",
    "loss = 0\n",
    "for q in range(num_val):    \n",
    "    test = extend_input(x_val[q],loc, scale, n_parts,noise_dim,shift_zeros=True)\n",
    "    z = encoder.model.predict(test,batch_size=(n_parts))\n",
    "    test_logp=decoder.eval_logpxz(z,x_val[q])\n",
    "    loss += np.mean(test_logp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
